# Refer√™ncia API

## load.py - Indexa√ß√£o de Documentos

### `load_documents()`

Carrega todos os arquivos PDF da pasta `data/`.

**Assinatura**:
```python
def load_documents() -> List[Document]
```

**Retorno**:
- `List[Document]`: Lista de documentos carregados
- `[]`: Lista vazia se pasta n√£o existe ou est√° vazia

**Comportamento**:
1. Verifica exist√™ncia da pasta `data/`
2. Lista arquivos `*.pdf`
3. Usa `DirectoryLoader` com `PyPDFLoader`
4. Mostra progress bar durante carregamento
5. Retorna documentos com metadata

**Exemplo**:
```python
documents = load_documents()
print(f"Carregados {len(documents)} documentos")
```

---

### `split_documents(documents)`

Divide documentos em chunks menores.

**Assinatura**:
```python
def split_documents(documents: List[Document]) -> List[Document]
```

**Par√¢metros**:
- `documents`: Lista de documentos a dividir

**Retorno**:
- `List[Document]`: Lista de chunks

**Configura√ß√£o**:
```python
RecursiveCharacterTextSplitter(
    chunk_size=1000,         # Tamanho do chunk
    chunk_overlap=200,       # Overlap entre chunks
    length_function=len,     # Fun√ß√£o de medida
    separators=["\n\n", "\n", " ", ""]  # Separadores hier√°rquicos
)
```

**Exemplo**:
```python
chunks = split_documents(documents)
print(f"Criados {len(chunks)} chunks")
```

---

### `create_embeddings()`

Configura OpenAI Embeddings API.

**Assinatura**:
```python
def create_embeddings() -> OpenAIEmbeddings
```

**Retorno**:
- `OpenAIEmbeddings`: Modelo de embeddings configurado

**Exce√ß√µes**:
- `ValueError`: Se `OPENAI_API_KEY` n√£o est√° definida

**Configura√ß√£o**:
```python
OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

**Exemplo**:
```python
embeddings = create_embeddings()
# Usar com Chroma ou outra vector store
```

---

### `create_vectorstore(chunks, embeddings)`

Cria e persiste Chroma vector store.

**Assinatura**:
```python
def create_vectorstore(
    chunks: List[Document],
    embeddings: OpenAIEmbeddings
) -> Chroma
```

**Par√¢metros**:
- `chunks`: Chunks de documentos
- `embeddings`: Modelo de embeddings

**Retorno**:
- `Chroma`: Vector store criado e persistido

**Diret√≥rio de persist√™ncia**: `vectorstore/`

**Exemplo**:
```python
vectorstore = create_vectorstore(chunks, embeddings)
print(f"Vector store criado em 'vectorstore/'")
```

---

## bot.py - Bot Discord

### Configura√ß√£o

#### `carregar_configuracoes()`

Carrega configura√ß√µes de servidores do JSON.

**Assinatura**:
```python
def carregar_configuracoes() -> dict
```

**Retorno**:
- `dict`: Dicion√°rio de configura√ß√µes
- `{}`: Dicion√°rio vazio se arquivo n√£o existe

**Estrutura do retorno**:
```python
{
    "123456789": {"nivel": "moderado"},
    "987654321": {"nivel": "liberal"}
}
```

---

#### `salvar_configuracoes(configs)`

Persiste configura√ß√µes no JSON.

**Assinatura**:
```python
def salvar_configuracoes(configs: dict) -> None
```

**Par√¢metros**:
- `configs`: Dicion√°rio de configura√ß√µes

**Efeitos**:
- Escreve `server_config.json` com encoding UTF-8
- Formata com `indent=2` para legibilidade
- Usa `ensure_ascii=False` para caracteres especiais

**Exemplo**:
```python
configs = carregar_configuracoes()
configs["123456789"] = {"nivel": "conservador"}
salvar_configuracoes(configs)
```

---

#### `obter_nivel_servidor(guild_id)`

Retorna n√≠vel de filtro configurado.

**Assinatura**:
```python
def obter_nivel_servidor(guild_id: Optional[int]) -> str
```

**Par√¢metros**:
- `guild_id`: ID do servidor (ou `None` para DM)

**Retorno**:
- `str`: "conservador", "moderado" (padr√£o), ou "liberal"

**Exemplo**:
```python
nivel = obter_nivel_servidor(123456789)
print(f"N√≠vel: {nivel}")  # "moderado"
```

---

#### `definir_nivel_servidor(guild_id, nivel)`

Define n√≠vel de filtro e registra log.

**Assinatura**:
```python
def definir_nivel_servidor(
    guild_id: Optional[int],
    nivel: str
) -> None
```

**Par√¢metros**:
- `guild_id`: ID do servidor (ou `None` para DM)
- `nivel`: "conservador", "moderado" ou "liberal"

**Efeitos**:
- Atualiza `server_config.json`
- Registra log: `"üìù Configura√ß√£o alterada | Servidor: {guild_id} | Novo n√≠vel: {nivel}"`

**Exemplo**:
```python
definir_nivel_servidor(123456789, "liberal")
```

---

### Processamento

#### `processar_pergunta(question, guild_id, user_id, tipo)`

Processa pergunta atrav√©s do pipeline RAG.

**Assinatura**:
```python
async def processar_pergunta(
    question: str,
    guild_id: Optional[int] = None,
    user_id: Optional[int] = None,
    tipo: str = "RAG"
) -> tuple[str, list]
```

**Par√¢metros**:
- `question`: Pergunta do usu√°rio
- `guild_id`: ID do servidor (opcional)
- `user_id`: ID do usu√°rio (opcional)
- `tipo`: Tipo de intera√ß√£o ("CMD /ask", "Men√ß√£o", "DM")

**Retorno**:
- `tuple[str, list]`: (resposta, fontes)
  - `resposta`: Texto da resposta gerada
  - `fontes`: Lista de `Document` com metadata

**Workflow**:
1. Verifica se RAG est√° carregado
2. Obt√©m n√≠vel de filtro do servidor
3. Seleciona prompt apropriado
4. Cria chain de recupera√ß√£o + gera√ß√£o
5. Invoca chain com pergunta
6. Extrai resposta e fontes
7. Registra logs

**Exce√ß√µes**:
- Retorna mensagem de erro se RAG n√£o carregado
- Captura exce√ß√µes e retorna erro formatado

**Logs**:
```
INFO: "üí¨ {tipo} | {guild_info} | Usu√°rio: {user_id} | N√≠vel: {nivel} | Pergunta: {question[:50]}..."
INFO: "‚úÖ Resposta enviada | {guild_info} | Usu√°rio: {user_id} | Fontes: {len(fontes)}"
ERROR: "‚ùå Erro ao processar | {guild_info} | Usu√°rio: {user_id} | Erro: {str(e)}"
```

**Exemplo**:
```python
resposta, fontes = await processar_pergunta(
    "O que √© RAG?",
    guild_id=123456789,
    user_id=987654321,
    tipo="CMD /ask"
)
```

---

#### `enviar_resposta_longa(channel, resposta, fontes)`

Divide e envia respostas longas.

**Assinatura**:
```python
async def enviar_resposta_longa(
    channel: discord.TextChannel,
    resposta: str,
    fontes: list
) -> None
```

**Par√¢metros**:
- `channel`: Canal Discord para enviar
- `resposta`: Texto da resposta
- `fontes`: Lista de documentos fonte

**Comportamento**:
1. Se `resposta <= 2000 chars`: Envia direto
2. Se `resposta > 2000 chars`: Divide em chunks de 2000
3. Envia cada chunk sequencialmente
4. Se houver fontes: Formata e envia (top 3)

**Formato de fontes**:
```
üìö Fontes:
1. `documento1.pdf`
2. `documento2.pdf`
3. `documento3.pdf`
```

**Exemplo**:
```python
await enviar_resposta_longa(
    message.channel,
    resposta_longa,
    fontes
)
```

---

### Eventos Discord

#### `on_ready()`

Executado quando bot conecta.

**Assinatura**:
```python
@bot.event
async def on_ready() -> None
```

**Efeitos**:
1. Log: `"ü§ñ Bot iniciado | Nome: {bot.user} | Servidores: {len(bot.guilds)}"`
2. Sincroniza slash commands
3. Log: `"‚öôÔ∏è Comandos sincronizados | Total: {len(synced)}"`
4. Captura e loga exce√ß√µes de sincroniza√ß√£o

---

#### `on_message(message)`

Processa mensagens recebidas.

**Assinatura**:
```python
@bot.event
async def on_message(message: discord.Message) -> None
```

**Par√¢metros**:
- `message`: Mensagem Discord

**Comportamento**:
1. **Ignora**: Mensagens do pr√≥prio bot
2. **Processa comandos**: `await bot.process_commands(message)`
3. **Men√ß√µes**: Se bot mencionado (n√£o @everyone)
   - Extrai pergunta
   - Chama `processar_pergunta()`
   - Envia resposta via `enviar_resposta_longa()`
4. **DMs**: Se canal √© DM
   - Chama `processar_pergunta()`
   - Envia resposta via `enviar_resposta_longa()`

**Logs**:
```
INFO: "üì© Men√ß√£o | Servidor: {guild_id} | Usu√°rio: {user_id}"
INFO: "üì® DM recebida | Usu√°rio: {user_id}"
```

---

#### `on_error(event, *args, **kwargs)`

Handler global de erros.

**Assinatura**:
```python
@bot.event
async def on_error(event: str, *args, **kwargs) -> None
```

**Par√¢metros**:
- `event`: Nome do evento
- `*args`: Argumentos do evento
- `**kwargs`: Keyword arguments do evento

**Efeitos**:
- Imprime erro no console
- Loga com stack trace completo: `"‚ùå Erro no evento {event} | Args: {args}"`

---

### Comandos Slash

#### `/ask`

Comando principal para fazer perguntas.

**Assinatura**:
```python
@bot.tree.command(name="ask", description="Faz uma pergunta ao RAG")
@app_commands.describe(pergunta="Sua pergunta")
async def ask(
    interaction: discord.Interaction,
    pergunta: str
) -> None
```

**Par√¢metros**:
- `interaction`: Intera√ß√£o Discord
- `pergunta`: Pergunta do usu√°rio

**Workflow**:
1. `await interaction.response.defer(thinking=True)`
2. Extrai `guild_id` e `user_id`
3. Log: `"üîπ Comando /ask | Servidor: {guild_id} | Usu√°rio: {user_id}"`
4. Chama `processar_pergunta()`
5. Envia resposta via `interaction.followup.send()`
6. Envia fontes (se houver)

---

#### `/config`

Configura n√≠vel de filtro (apenas admins).

**Assinatura**:
```python
@bot.tree.command(name="config", description="Configura o n√≠vel de filtro")
@app_commands.describe(nivel="Escolha o n√≠vel")
@app_commands.choices(nivel=[...])
async def config(
    interaction: discord.Interaction,
    nivel: app_commands.Choice[str]
) -> None
```

**Par√¢metros**:
- `interaction`: Intera√ß√£o Discord
- `nivel`: Choice (conservador, moderado, liberal)

**Valida√ß√£o**:
1. Se servidor: Verifica se usu√°rio √© administrador
2. Se n√£o admin: Retorna erro ephemeral

**Workflow**:
1. Log: `"üîπ Comando /config | Servidor: {guild_id} | Usu√°rio: {user_id} | Tentativa: {nivel.value}"`
2. Valida permiss√µes
3. Se negado: Log WARNING
4. Se autorizado: Chama `definir_nivel_servidor()`
5. Envia confirma√ß√£o

---

#### `/status`

Mostra configura√ß√µes atuais.

**Assinatura**:
```python
@bot.tree.command(name="status", description="Mostra configura√ß√µes")
async def status(interaction: discord.Interaction) -> None
```

**Par√¢metros**:
- `interaction`: Intera√ß√£o Discord

**Workflow**:
1. Log: `"üîπ Comando /status | Servidor: {guild_id} | Usu√°rio: {user_id}"`
2. Obt√©m n√≠vel atual via `obter_nivel_servidor()`
3. Cria embed com informa√ß√µes:
   - N√≠vel de filtro
   - Modelo LLM
   - Status do RAG
4. Envia embed

**Embed**:
```
‚öôÔ∏è Configura√ß√µes do Bot
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
N√≠vel de Filtro: ‚öñÔ∏è MODERADO
Modelo LLM: minimax/minimax-m2:free
RAG Status: ‚úÖ Ativo
```

---

## Constantes

### `DISCORD_TOKEN`
Token do bot Discord (vari√°vel de ambiente).

### `OPENAI_API_KEY`
Chave API OpenAI para embeddings (vari√°vel de ambiente).

### `OPENROUTER_API_KEY`
Chave API OpenRouter para LLM (vari√°vel de ambiente).

### `OPENROUTER_MODEL`
Modelo LLM a usar (vari√°vel de ambiente, padr√£o: `"anthropic/claude-3.5-sonnet"`).

### `INDEX_PATH`
Caminho do vector store (`"vectorstore"`).

### `K_DOCS`
N√∫mero de documentos a recuperar (padr√£o: `5`).

### `CONFIG_FILE`
Arquivo de configura√ß√µes (`"server_config.json"`).

### `PROMPTS_POR_NIVEL`
Dicion√°rio com prompts para cada n√≠vel:
```python
{
    "conservador": "Prompt formal...",
    "moderado": "Prompt equilibrado...",
    "liberal": "Prompt casual..."
}
```

---

## Logging

### Configura√ß√£o

```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        RotatingFileHandler(
            'logs/bot.log',
            maxBytes=5*1024*1024,
            backupCount=5,
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
```

### Logger

```python
logger = logging.getLogger('SamiraBot')
```

### M√©todos

#### `logger.info(message)`
Registra evento informativo.

#### `logger.warning(message)`
Registra aviso.

#### `logger.exception(message)`
Registra erro com stack trace completo.
