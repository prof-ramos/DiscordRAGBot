# Arquitetura

## VisÃ£o Geral do Sistema

O projeto implementa um pipeline RAG (Retrieval-Augmented Generation) completo:

```
PDFs â†’ IndexaÃ§Ã£o â†’ Vector Store â†’ RecuperaÃ§Ã£o â†’ LLM â†’ Resposta
```

## Componentes Principais

### 1. Sistema de IndexaÃ§Ã£o (`load.py`)

ResponsÃ¡vel por processar documentos PDF e criar o banco vetorial.

**Workflow**:
1. Carrega PDFs da pasta `data/`
2. Divide em chunks (1000 chars, overlap 200)
3. Gera embeddings via OpenAI API
4. Persiste no Chroma vector store

**CaracterÃ­sticas**:
- Suporta mÃºltiplos PDFs simultaneamente
- Progress bar durante indexaÃ§Ã£o
- ValidaÃ§Ã£o de diretÃ³rios e arquivos

### 2. Bot Discord (`bot.py`)

AplicaÃ§Ã£o principal que integra Discord e RAG.

**Responsabilidades**:
- Carrega vector store Chroma
- Processa interaÃ§Ãµes Discord (slash commands, menÃ§Ãµes, DMs)
- Recupera contexto relevante via busca vetorial
- Gera respostas via LLM (OpenRouter)
- Gerencia configuraÃ§Ãµes por servidor
- Sistema completo de logging

### 3. Sistema de ConfiguraÃ§Ã£o

Gerencia personalizaÃ§Ã£o por servidor.

**Arquivo**: `server_config.json`  
**Estrutura**:
```json
{
  "123456789": {
    "nivel": "moderado"
  },
  "987654321": {
    "nivel": "liberal"
  }
}
```

**NÃ­veis disponÃ­veis**:
- **conservador**: Formal, profissional, neutro
- **moderado**: Equilibrado, empÃ¡tico, informativo (padrÃ£o)
- **liberal**: Casual, descontraÃ­do, autÃªntico

### 4. Sistema de Logs

Sistema robusto de logging com rotaÃ§Ã£o automÃ¡tica.

**CaracterÃ­sticas**:
- RotatingFileHandler (5MB max, 5 backups)
- Formato estruturado: `timestamp | nÃ­vel | mensagem`
- Stack traces completos em erros
- UTF-8 encoding

**LocalizaÃ§Ã£o**: `logs/bot.log`

## Fluxo de Dados

### Pipeline RAG Completo

```mermaid
graph TB
    subgraph Discord["ğŸ® Discord"]
        User[ğŸ‘¤ UsuÃ¡rio]
        Bot[ğŸ¤– Bot Discord]
    end
    
    subgraph Processing["âš™ï¸ Processamento"]
        Parse[Processar<br/>Pergunta]
        Config[Obter NÃ­vel<br/>do Servidor]
        Prompt[Selecionar<br/>Prompt]
    end
    
    subgraph RAG["ğŸ” Sistema RAG"]
        Embed[Embedding<br/>da Pergunta]
        Search[Busca Vetorial<br/>Chroma]
        Retrieve[Top K<br/>Documentos]
    end
    
    subgraph LLM["ğŸ§  GeraÃ§Ã£o"]
        Context[Contexto +<br/>Pergunta]
        Generate[LLM<br/>OpenRouter]
        Response[Resposta<br/>Gerada]
    end
    
    subgraph Storage["ğŸ’¾ Armazenamento"]
        Vector[(Vector Store<br/>Chroma)]
        ServerConf[(server_config.json)]
        Logs[(Logs)]
    end
    
    User -->|/ask, menÃ§Ã£o, DM| Bot
    Bot --> Parse
    Parse --> Config
    Config --> ServerConf
    Config --> Prompt
    Prompt --> Embed
    Embed --> Search
    Search --> Vector
    Search --> Retrieve
    Retrieve --> Context
    Prompt --> Context
    Context --> Generate
    Generate --> Response
    Response --> Bot
    Bot -->|Resposta + Fontes| User
    Bot -.->|Registra| Logs
    
    style User fill:#4CAF50,stroke:#2E7D32,color:#fff
    style Bot fill:#5865F2,stroke:#4752C4,color:#fff
    style Vector fill:#9C27B0,stroke:#7B1FA2,color:#fff
    style Generate fill:#FF6B6B,stroke:#C92A2A,color:#fff
    style Response fill:#4CAF50,stroke:#2E7D32,color:#fff
```

### Processamento de Pergunta - SequÃªncia Temporal

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ UsuÃ¡rio
    participant D as Discord
    participant B as Bot
    participant C as Config
    participant V as Vector Store
    participant E as OpenAI Embeddings
    participant L as OpenRouter LLM
    participant Log as Sistema Logs
    
    U->>D: /ask pergunta: Como funciona RAG?
    D->>B: Interaction Event
    B->>Log: ğŸ“ Comando recebido
    
    B->>C: Obter nÃ­vel do servidor
    C-->>B: "moderado"
    
    B->>E: Gerar embedding da pergunta
    E-->>B: Vetor [1536-D]
    
    B->>V: Busca por similaridade (K=5)
    V-->>B: Top 5 documentos relevantes
    
    B->>L: Prompt + Contexto + Pergunta
    Note over L: Gera resposta usando<br/>Claude/GPT/Gemini
    L-->>B: Resposta gerada
    
    B->>Log: âœ… Resposta enviada + fontes
    B->>D: Resposta + ğŸ“š Fontes
    D->>U: Mensagem com resposta
```

**Etapas Detalhadas:**

1. **Recebimento**: Discord (comando, menÃ§Ã£o ou DM)
2. **ValidaÃ§Ã£o**: Verifica se RAG estÃ¡ carregado
3. **ConfiguraÃ§Ã£o**: ObtÃ©m nÃ­vel de filtro do servidor
4. **RecuperaÃ§Ã£o**: Busca K documentos mais relevantes (padrÃ£o: 5)
5. **GeraÃ§Ã£o**: LLM cria resposta usando contexto
6. **Resposta**: Envia ao usuÃ¡rio com fontes

### Sistema de Prompts

O bot utiliza prompts diferentes baseados no nÃ­vel configurado:

```python
PROMPTS_POR_NIVEL = {
    "conservador": "Prompt formal e profissional...",
    "moderado": "Prompt equilibrado e empÃ¡tico...",
    "liberal": "Prompt casual e autÃªntico..."
}
```

Cada prompt Ã© injetado dinamicamente no chain do LangChain.

## Tecnologias e Bibliotecas

### Core

- **Python 3.11** - Linguagem base
- **discord.py** - Framework Discord Bot
- **LangChain 1.0** - OrquestraÃ§Ã£o RAG

### RAG Pipeline

- **Chroma** - Banco de dados vetorial
- **OpenAI Embeddings API** - text-embedding-3-small
- **OpenRouter** - Gateway para LLMs (Claude, GPT, etc.)

### Processamento de Documentos

- **pypdf** - Leitura de PDFs
- **RecursiveCharacterTextSplitter** - DivisÃ£o de texto

### Infraestrutura

- **python-dotenv** - Gerenciamento de variÃ¡veis
- **logging** - Sistema de logs
- **json** - PersistÃªncia de configuraÃ§Ãµes

## Estrutura de DiretÃ³rios

```
.
â”œâ”€â”€ data/                    # PDFs para indexaÃ§Ã£o
â”œâ”€â”€ vectorstore/             # Chroma DB (gerado)
â”œâ”€â”€ logs/                    # Logs rotativos
â”‚   â”œâ”€â”€ bot.log
â”‚   â”œâ”€â”€ bot.log.1
â”‚   â””â”€â”€ bot.log.2
â”œâ”€â”€ docs/                    # DocumentaÃ§Ã£o MkDocs
â”œâ”€â”€ load.py                  # Script de indexaÃ§Ã£o
â”œâ”€â”€ bot.py                   # Bot Discord
â”œâ”€â”€ requirements.txt         # DependÃªncias
â”œâ”€â”€ server_config.json       # Configs por servidor
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente
â””â”€â”€ mkdocs.yml              # ConfiguraÃ§Ã£o MkDocs
```

## DecisÃµes de Design

### Por que Chroma?

- **Leveza**: Menor uso de disco vs FAISS
- **Facilidade**: API simples e direta
- **PersistÃªncia**: Salva automaticamente em disco
- **IntegraÃ§Ã£o**: Suporte nativo LangChain

### Por que OpenAI Embeddings?

- **Qualidade**: Excelente para portuguÃªs
- **Custo**: ~$0.02 por 1M tokens (muito baixo)
- **Simplicidade**: API vs modelo local (economiza disco)
- **MultilÃ­ngue**: Otimizado para mÃºltiplos idiomas

### Por que OpenRouter?

- **Flexibilidade**: Acesso a mÃºltiplos modelos
- **Economia**: Permite escolher modelo por custo
- **Simplicidade**: API Ãºnica para vÃ¡rios LLMs
- **Gratuito**: Modelos free disponÃ­veis

## Escalabilidade

### LimitaÃ§Ãµes Atuais

- **Single-threaded**: Processa uma pergunta por vez
- **In-memory**: Vector store carregado em RAM
- **Sem cache**: NÃ£o armazena respostas anteriores

### Melhorias Futuras

- Rate limiting por usuÃ¡rio
- Cache de respostas frequentes
- Processamento paralelo de perguntas
- Sharding do vector store
- MÃ©tricas e analytics

## SeguranÃ§a

### ProteÃ§Ãµes Implementadas

âœ… **Secrets**: VariÃ¡veis de ambiente (nunca hardcoded)  
âœ… **PermissÃµes**: Controle de acesso `/config` (sÃ³ admins)  
âœ… **Logs**: NÃ£o expÃµem tokens ou chaves  
âœ… **ValidaÃ§Ã£o**: Input sanitization em comandos

### ConsideraÃ§Ãµes

âš ï¸ **Vector Store**: ContÃ©m texto dos PDFs (pode ter dados sensÃ­veis)  
âš ï¸ **Logs**: Registram User IDs e Guild IDs  
âš ï¸ **ConfiguraÃ§Ãµes**: `server_config.json` tem IDs de servidores
